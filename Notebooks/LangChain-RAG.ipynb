{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b385822",
   "metadata": {},
   "source": [
    "# LangChain - Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443b176",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this notebook, we demonstrate LangChain with an example involving RAG, input/output parsing, output formatting, etc.\n",
    "\n",
    "**RAG-based Question-Answering System using LangChain**\n",
    "\n",
    "In this example, we use a web page as our knowledge source and build a chain that can answer questions about its content.\n",
    "\n",
    "* Data Source – https://lilianweng.github.io/posts/2023-06-23-agent/​\n",
    "\n",
    "* LLM –  Gpt-4o-mini ​\n",
    "\n",
    "* Vector Store – ChromaDB​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50aaeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f5cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt') as ap:\n",
    "    api_key = ap.read()\n",
    "#     print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc3fccd",
   "metadata": {},
   "source": [
    "### Load Data and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd33be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/dgkwpqzd10xgjv9hflydy7tm0000gn/T/ipykernel_9917/1523640762.py:12: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=api_key))\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Load the web page\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db01ce",
   "metadata": {},
   "source": [
    "### Setup Retriever and RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6183cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/dgkwpqzd10xgjv9hflydy7tm0000gn/T/ipykernel_9917/3461173783.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(api_key=api_key, model_name=\"gpt-4o-mini\", temperature=0.3)\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Custom prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following format:\n",
    "Answer: [Your answer here]\n",
    "Source: [Page title or URL of the source]\n",
    "Confidence: [High/Medium/Low]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG chain\n",
    "model = ChatOpenAI(api_key=api_key, model_name=\"gpt-4o-mini\", temperature=0)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234d74e",
   "metadata": {},
   "source": [
    "### Custom Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da312b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CustomOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        result = {}\n",
    "        for line in lines:\n",
    "            if \": \" in line:\n",
    "                key, value = line.split(\": \", 1)\n",
    "                result[key.lower()] = value.strip()\n",
    "            else:\n",
    "                # Handle cases where there's no colon separator\n",
    "                result['content'] = line.strip()\n",
    "        return result\n",
    "    \n",
    "output_parser = CustomOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17904b66",
   "metadata": {},
   "source": [
    "### Final Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6ccd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "def format_output(parsed_output):\n",
    "    return f\"\"\"\n",
    "## Question Answered\n",
    "Answer: {parsed_output['answer']}\n",
    "\n",
    "**Source:** {parsed_output['source']}\n",
    "**Confidence:** {parsed_output['confidence']}\n",
    "\"\"\"\n",
    "\n",
    "final_chain = rag_chain | output_parser | format_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e610a",
   "metadata": {},
   "source": [
    "### Query and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58701bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Question: \n",
      " What is task decomposition in the context of AI agents?\n",
      "\n",
      "## Question Answered\n",
      "Answer: Task decomposition in the context of AI agents refers to the process of breaking down complex tasks into smaller, manageable subgoals. This allows the agent to handle complicated tasks more efficiently. Techniques such as Chain of Thought (CoT) prompting are used to guide the model in thinking step by step, transforming larger tasks into simpler ones, and providing insights into the model's reasoning process. Additionally, task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.\n",
      "\n",
      "**Source:** LLM Powered Autonomous Agents | Lil'Log\n",
      "**Confidence:** High\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition in the context of AI agents?\"\n",
    "result = final_chain.invoke(question)\n",
    "print(f\"##Question: \\n {question}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2903ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Question: \n",
      " Explain self-reflection in the context of AI agents?\n",
      "\n",
      "## Question Answered\n",
      "Answer: Self-reflection in the context of AI agents is a vital aspect that allows these agents to improve iteratively by refining past action decisions and correcting previous mistakes. It involves the agent analyzing its past actions, learning from failures, and making adjustments for future tasks. This process is crucial in real-world scenarios where trial and error are common, enabling the agent to enhance its performance over time. In frameworks like Reflexion, self-reflection is facilitated by showing the agent examples of failed trajectories alongside ideal reflections, which are then stored in the agent's working memory to guide future actions.\n",
      "\n",
      "**Source:** LLM Powered Autonomous Agents | Lil'Log (https://lilianweng.github.io/posts/2023-06-23-agent/)\n",
      "**Confidence:** High\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain self-reflection in the context of AI agents?\"\n",
    "result = final_chain.invoke(question)\n",
    "print(f\"##Question: \\n {question}\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
